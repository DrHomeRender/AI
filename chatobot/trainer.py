import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset

# âœ… 1. KoGPT2 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "skt/kogpt2-base-v2"
tokenizer = GPT2TokenizerFast.from_pretrained(
    "skt/kogpt2-base-v2",
    bos_token="</s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    mask_token="<mask>",
)

# âœ… 2. Padding Token ì„¤ì • (GPT2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ padding tokenì´ ì—†ìŒ)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    tokenizer.pad_token = tokenizer.eos_token  # ë˜ëŠ” '[PAD]' ì‚¬ìš© ê°€ëŠ¥

# âœ… 3. ëª¨ë¸ ë¡œë“œ ë° Token Embeddings ì¡°ì •
model = GPT2LMHeadModel.from_pretrained(model_name)
model.resize_token_embeddings(len(tokenizer))  # ì¶”ê°€ëœ PAD í† í° ë°˜ì˜

# âœ… 4. ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("text", data_files={"train": "gpt2_train_data.txt"})

# ğŸ” ë””ë²„ê¹…: ë°ì´í„°ì…‹ í™•ì¸
print("\nğŸ”¹ ì›ë³¸ ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸:")
print(dataset["train"][0])  # ì²« ë²ˆì§¸ ë°ì´í„° í™•ì¸

# âœ… 5. ë°ì´í„° í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬)
def tokenize_function(examples):
    texts = examples["text"]  # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ì˜¨ ë¬¸ì¥ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬

    # ğŸ” ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° í™•ì¸
    print("\nğŸ”¹ tokenize_function ì…ë ¥ ì˜ˆì œ:")
    print(texts)

    # âœ… ê°œë³„ ë¬¸ì¥ì— ëŒ€í•´ Tokenize ìˆ˜í–‰
    tokenized_texts = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    # ğŸ” ë””ë²„ê¹…: í† í°í™”ëœ ê²°ê³¼ í™•ì¸
    print("\nğŸ”¹ í† í°í™”ëœ ë°ì´í„° ìƒ˜í”Œ:")
    print(tokenized_texts)

    return tokenized_texts

# âœ… 6. ë°ì´í„°ì…‹ ë³€í™˜ (í† í¬ë‚˜ì´ì§• ì ìš©)
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# ğŸ” ë””ë²„ê¹…: ë³€í™˜ëœ ë°ì´í„°ì…‹ í™•ì¸
print("\nğŸ”¹ ë³€í™˜ëœ ë°ì´í„°ì…‹ ìƒ˜í”Œ:")
print(tokenized_datasets["train"][0])

# âœ… 7. ë°ì´í„° Collator ì„¤ì • (Padding ì ìš©)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# âœ… 8. í›ˆë ¨ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./kogpt2_results",
    num_train_epochs=5,  # í•™ìŠµ Epochs
    per_device_train_batch_size=4,  # ë°°ì¹˜ í¬ê¸°
    per_device_eval_batch_size=4,
    save_strategy="epoch",  # ë§¤ Epoch ë§ˆë‹¤ ì €ì¥
    evaluation_strategy="no",  # í‰ê°€ ìˆ˜í–‰ ì•ˆ í•¨ (ì˜¤ë¥˜ í•´ê²°)
    logging_dir="./kogpt2_logs",
    logging_steps=50,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=100,
    lr_scheduler_type="cosine",
    save_total_limit=2,
    load_best_model_at_end=False,  # ğŸ”¹ í‰ê°€ ì—†ì´ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥ ì•ˆ í•¨ (ì˜¤ë¥˜ í•´ê²°)
    report_to="none"
)
# âœ… 9. Trainer ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    data_collator=data_collator
)

# âœ… 10. í›ˆë ¨ ì‹œì‘
trainer.train()

# âœ… 11. í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./trained_kogpt2")
tokenizer.save_pretrained("./trained_kogpt2")
print("âœ… KoGPT2 í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!")
