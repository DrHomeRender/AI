import torch
import torch.nn as nn
import torch.optim as optim
import os
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from transformer import TransformerModel
from data_process import load_data, preprocess_data, split_data

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
BATCH_SIZE = 32
EPOCHS = 1000
LEARNING_RATE = 0.001
MODEL_BEST_PATH = "best_model.pth"  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
MODEL_LATEST_PATH = "latest_model.pth"  # ìµœì‹  ëª¨ë¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
file_path = "data_season.csv"
df = load_data(file_path)
df, encoders, scaler, target_scaler = preprocess_data(df)  # target_scaler ì¶”ê°€
X_train, X_test, y_train, y_test = split_data(df)

# PyTorch Tensor ë³€í™˜
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)

# ë°ì´í„° ë¡œë” ìƒì„±
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ëª¨ë¸ ì´ˆê¸°í™”
input_dim = X_train.shape[1]
output_dim = y_train.shape[1]
model = TransformerModel(input_dim=input_dim, output_dim=output_dim).to(device)

# ê¸°ì¡´ ëª¨ë¸ì´ ì¡´ì¬í•˜ë©´ ë¶ˆëŸ¬ì˜¤ê¸°
best_loss = float("inf")
if os.path.exists(MODEL_LATEST_PATH):
    model.load_state_dict(torch.load(MODEL_LATEST_PATH, map_location=device))
    print("ê¸°ì¡´ ìµœì‹  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì¶”ê°€ í•™ìŠµ ì§„í–‰í•©ë‹ˆë‹¤.")
else:
    print("ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±!")

# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# í•™ìŠµ ê³¼ì • ì‹œê°í™”ë¥¼ ìœ„í•œ ë³€ìˆ˜
losses = []


# í•™ìŠµ ë£¨í”„
def train():
    global best_loss
    model.train()

    for epoch in range(EPOCHS):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x.unsqueeze(1))

            loss = criterion(outputs.squeeze(1), batch_y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        losses.append(avg_loss)  # Loss ì €ì¥
        print(f"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.4f}")

        # Best ëª¨ë¸ ì €ì¥ (ìµœì € Loss ê°±ì‹  ì‹œ)
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), MODEL_BEST_PATH)
            print(f"âœ… ìƒˆë¡œìš´ Best ëª¨ë¸ ì €ì¥ (Loss: {best_loss:.4f})")

        # ìµœì‹  ëª¨ë¸ ì €ì¥
        torch.save(model.state_dict(), MODEL_LATEST_PATH)

    print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")

    # Loss ê·¸ë˜í”„ ì €ì¥
    plt.figure(figsize=(10, 5))
    plt.plot(losses, label="Train Loss", color="blue")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training Loss Curve")
    plt.legend()
    plt.grid()
    plt.savefig("loss_curve.png")
    print("ğŸ“Š í•™ìŠµ Loss ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ (loss_curve.png)")


if __name__ == "__main__":
    train()
