import torch
import torch.nn as nn
import torch.optim as optim
import os
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from transformer import TransformerModel
from data_transform import load_data, preprocess_data  # âœ… ë°ì´í„° ë³€í™˜ ì½”ë“œ ì‚¬ìš©

print("GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€:", torch.cuda.is_available())
print("ì‚¬ìš© ì¤‘ì¸ GPU:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")
print("í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤:", torch.cuda.current_device())

# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
BATCH_SIZE = 32
EPOCHS = 1000
LEARNING_RATE = 0.001
MODEL_BEST_PATH = "best.pth"
MODEL_LATEST_PATH = "latest.pth"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
file_path = "data_season.csv"
df = load_data(file_path)
X_train, X_test, y_train, y_test, encoders, scaler = preprocess_data(df)

# âœ… PyTorch Tensor ë³€í™˜
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)

# âœ… ë°ì´í„° ë¡œë” ìƒì„± (ìµœì í™”)
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(
    train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=os.cpu_count()
)
test_loader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=os.cpu_count()
)

# âœ… ëª¨ë¸ ì´ˆê¸°í™”
input_dim = X_train.shape[1]  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©ìœ¼ë¡œ input_dim ì¦ê°€
output_dim = y_train.shape[1]  # ìˆ˜í™•ëŸ‰ê³¼ ê°€ê²© ì˜ˆì¸¡ (2ê°œ ì¶œë ¥)
model = TransformerModel(input_dim=input_dim, output_dim=output_dim).to(device)

# âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
best_loss = float("inf")
if os.path.exists(MODEL_LATEST_PATH):
    model.load_state_dict(torch.load(MODEL_LATEST_PATH, map_location=device))
    print("ğŸ”„ ê¸°ì¡´ ìµœì‹  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì¶”ê°€ í•™ìŠµ ì§„í–‰í•©ë‹ˆë‹¤.")
else:
    print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±!")

# âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
criterion = nn.MSELoss()  # âœ… MSELoss ì‚¬ìš© (SmoothL1Lossë„ ê°€ëŠ¥)
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)  # âœ… AdamW ì ìš©
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)  # âœ… ë™ì  í•™ìŠµë¥  ì¡°ì ˆ

# âœ… í•™ìŠµ ê³¼ì • ì‹œê°í™”ë¥¼ ìœ„í•œ ë³€ìˆ˜
losses = []

# âœ… í•™ìŠµ ë£¨í”„
def train():
    global best_loss
    model.train()

    for epoch in range(EPOCHS):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device, non_blocking=True), batch_y.to(device, non_blocking=True)

            optimizer.zero_grad()
            outputs = model(batch_x.unsqueeze(1))  # âœ… Transformer ì…ë ¥ ì°¨ì› ë§ì¶”ê¸°
            loss = criterion(outputs.squeeze(1), batch_y)
            loss.backward()
            
            # âœ… Gradient Clipping ì ìš© (ê¸°ìš¸ê¸° í­ë°œ ë°©ì§€)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        losses.append(avg_loss)  # âœ… Loss ì €ì¥
        print(f"ğŸ“ Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.6f}")

        # âœ… Best ëª¨ë¸ ì €ì¥ (ìµœì € Loss ê°±ì‹  ì‹œ)
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), MODEL_BEST_PATH)
            print(f"âœ… ìƒˆë¡œìš´ Best ëª¨ë¸ ì €ì¥ (Loss: {best_loss:.6f})")

        # âœ… ìµœì‹  ëª¨ë¸ ì €ì¥ (10 Epochë§ˆë‹¤)
        if (epoch + 1) % 10 == 0:
            torch.save(model.state_dict(), MODEL_LATEST_PATH)

        # âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (Loss ê¸°ë°˜ ê°ì†Œ)
        scheduler.step(avg_loss)

    print("âœ… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")

    # âœ… Loss ê·¸ë˜í”„ ì €ì¥
    plt.figure(figsize=(10, 5))
    plt.plot(losses, label="Train Loss", color="blue")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training Loss Curve")
    plt.legend()
    plt.grid()
    plt.savefig("loss_curve.png")
    print("ğŸ“Š í•™ìŠµ Loss ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ (loss_curve.png)")


if __name__ == "__main__":
    train()
